---
title: "Ağaç Bazlı Yöntemler"
author: "Prof. Dr. Hüseyin Taştan"
date: "Ağustos 2020"
output:
  html_document: 
    number_sections: true
    theme: readable
    highlight: haddock 
    # code_folding: show
    toc: yes
    toc_depth: 3
    toc_float: yes
    keep_md: true
subtitle: (İktisatçılar İçin) Makine Öğrenmesi (TEK-ES-2020)
institute: Yıldız Teknik Üniversitesi


---
<style type="text/css"> 
body{
  font-size: 12pt;
}
code.r{
  font-size: 12pt;
}
</style>

```{r setup, include=FALSE}
# knitr::opts_chunk$set(echo = TRUE, results = 'asis', fig.show = 'asis')
knitr::opts_chunk$set(echo = TRUE)
``` 

<br/>
<br/>

# R ile Sınıflandırma Ağaçları
 
`R`'da ağaç tahmini için çok sayıda paket vardır. Bunlardan biri hem regresyon hem de sınıflama ağaçlarının tahminini yapan `tree` paketidir. Burada örnek olarak  `Carseats` veri setini kullanacağız. `Sales` değişkeni 8 değeri kullanılarak ikili kategorik değişkene dönüştürülmüştür: 
```{r, echo=TRUE, results="hold", warning=FALSE}
library(tree)
library(ISLR)
attach(Carseats)
High <- ifelse(Sales<=8, "No", "Yes")
Carseats <- data.frame(Carseats, High)
```

`High` Satışlar 8'den büyükse "Yes" (1) değilse "No" (0) değerini almaktadır. 

`Sales` değişkenini dışlayarak diğer tüm değişkenlerle `High` için bir ağaç tahmin edelim: 
```{r}
tree.carseats <- tree(High ~ . -Sales, data = Carseats)
summary(tree.carseats) 
```

Eğitim hata oranı %9 olarak bulunmuştur (misclassification error rate).

Tahmin edilen ağacın yaprak sayısı 27'dir (terminal node): 
```{r, echo=TRUE, results="hold"}
plot(tree.carseats)
text(tree.carseats, pretty=0)
```

Satışların yüksekliğni etkileyen en önemli değişken raf yeridir (`ShelveLoc`, **B**ad, **G**ood, **M**edium). İlk düğümde Kötü ve Orta kalitedeki raf lokasyonları İyi raf lokasyonundan ayrılmaktadır. 

Ağacın tüm detayları: 
```{r}
tree.carseats
```


Test hata oranı: 
```{r}
set.seed(2)
train <- sample(1:nrow(Carseats), 200)
Carseats.test <- Carseats[-train,]
High.test <- High[-train]
tree.carseats <- tree(High ~ . -Sales, Carseats, subset = train)
tree.pred <- predict(tree.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
```

```{r, echo=TRUE, results="hold"}
# correct prediction rate
(104+50)/200
```

Ağacın çapraz geçerleme ile budanması,  `cv.tree()` fonksiyonu: 
```{r} 
set.seed(11)
cv.carseats <- cv.tree(tree.carseats, FUN=prune.misclass)
names(cv.carseats)
cv.carseats
```

Yukarıdaki çıktıda, `dev` çapraz geçerleme hatasını, `k` ayarlama parametresi $\alpha$'yı ve `size` ağacın büyüklüğünü (terminal düğüm sayısı) göstermektedir. En küçük çapraz geçerleme hatası 74'dür ve yaprak sayısı 8 olan bir ağaca karşılık gelmektedir.  

Ağaç büyüklüğü ve k (cost-complexity parameter)'ye göre hata oranının grafikleri: 
```{r}
par(mfrow=c(1,2))
plot(cv.carseats$size, cv.carseats$dev, type="b")
plot(cv.carseats$k, cv.carseats$dev, type="b")
```

Şimde `prune.misclass()` fonksiyonunu kullanarak ağacı budayabiliriz: 
```{r}
prune.carseats <- prune.misclass(tree.carseats, best=8)
plot(prune.carseats)
text(prune.carseats, pretty=0)
```

Not: kitapta 9 yapraklı bir ağaç tahmin edilmiştir: 
```{r}
prune.carseats <- prune.misclass(tree.carseats, best=9)
plot(prune.carseats)
text(prune.carseats,pretty=0)
```

Budanmış ağacın kestirim performansı nasıldır? `predict()` fonksiyonuyla sınıflandırma hatasını tahmin edelim:
```{r}
tree.pred <- predict(prune.carseats, Carseats.test, type="class")
table(tree.pred, High.test)
(97+58)/200
```

<br/>

# R ile Regresyon Ağaçları

Örnek olarak `MASS` paketinde yer alan `Boston` ev fiyatları verisini kullanacağız. Bağımlı değişken medyan ev fiyatları (`medv`). Eğitim verilerinde ağaç tahmini: 
```{r}
library(MASS)
set.seed(81)
train <- sample(1:nrow(Boston), nrow(Boston)/2)
tree.boston <- tree(medv ~ ., Boston, subset = train)
summary(tree.boston)
```

Yukarıdaki çıktıya göre ağaçta 4 değişken year alıyor: "rm"    "lstat" "crim"  "age"  

```{r}
plot(tree.boston)
text(tree.boston, pretty=0)
```

`lstat`: sosyoekonomik statüsü düşük olan hanelerin oranı. Bu değişkenin düşük olduğu yerlerde evler daha pahalıdır. 


Ağacın budanması: 
```{r}
cv.boston <- cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type='b')
```

Çapraz geçerleme en karmaşık ağacı seçti. Yine de ağacı budamak istersek `prune.tree()` fonksiyonunu kullanabiliriz:
```{r}
prune.boston <- prune.tree(tree.boston, best=5)
plot(prune.boston)
text(prune.boston, pretty=0)
```

Budanmamış ağaç üzerinden test hatasının tahmini: 
```{r}
yhat <- predict(tree.boston, newdata = Boston[-train,])
boston.test <- Boston[-train, "medv"]
plot(yhat,boston.test)
abline(0,1)
mean((yhat-boston.test)^2)
```
Test MSE = 18.93.  


<br/>

# Bagging ve Rassal Ormanlar 

Örnek veri seti = `Boston`. Bagging ve rassal ormanlar için R `randomForest` 
paketi kullanılabilir. Bagging rassal ormanların özel bir haliydi (m = p). Bagging tahmini: 
```{r, warning=FALSE}
library(randomForest)
set.seed(1)
bag.boston <- randomForest(medv ~ ., data = Boston, subset=train, mtry=13, importance=TRUE)
bag.boston
```

`mtry=13` opsiyonu: 13 değişkenin tamamı her ayırımda dikkate alınacak (bagging). Modelin test setindeki performansı: 
```{r}
yhat.bag <- predict(bag.boston, newdata = Boston[-train,])
plot(yhat.bag, boston.test)
abline(0,1)
mean((yhat.bag-boston.test)^2)
```

Test MSE = 10.13872. 

`randomForest()` fonksiyonunda `ntree` opsiyonu ile ağacın büyüklüğünü değiştirebiliriz:
```{r}
bag.boston <- randomForest(medv ~ ., data = Boston, subset = train, mtry=13, ntree=25)
yhat.bag <- predict(bag.boston, newdata=Boston[-train,])
mean((yhat.bag-boston.test)^2)
```

Rassal orman tahmini de benzer adımalara sahiptir. Ancak bu durumda daha küçük bir `mtry` değeri kullanırız. Default olarak `randomForest()` fonksiyonu $p/3$ değişkeni kullanır (sınıflama için 
$\sqrt{p}$). Aşağıda `mtry = 6` kullanılmıştır: 
```{r}
set.seed(1)
rf.boston <- randomForest(medv ~ ., data=Boston, subset=train, mtry=6, importance=TRUE)
yhat.rf <- predict(rf.boston, newdata=Boston[-train,])
mean((yhat.rf-boston.test)^2)
```

Rassal ormanlar küçük de olsa daha iyi bir test başarısı sergiledi. 

`importance()` fonksiyonunu kullanarak her bir değişkenin önem düzeyini görebiliriz:
```{r}
importance(rf.boston)
```

```{r}
varImpPlot(rf.boston)
```

Sonuçlara göre bölgenin refah düzeyi (`lstat`) ve ev büyüklüğü (`rm`) en önemli değişkenlerdir. 

<br/>

# Boosting 

Boosting (takviye) yöntemi için `gbm` paketindeki `gbm()` fonksiyonu kullanılabilir. Örnek olarak `Boston` veri seti için bir takviyeli ağaç tahmin edelim. Bunun için  `gbm()` fonksiyonunu `distribution="gaussian"` opsiyonu ile çalıştıracağız (regresyon problemi olduğu için). İkili sınıflama problemleri için `distribution="bernoulli"` kullanılabilir. `n.trees=5000` opsiyonu 5000 ağaç oluşturulacağını, `interaction.depth=4` her ağacın derinliğini belirtmektedir.  
```{r, warning=FALSE}
library(gbm)
set.seed(1)
boost.boston <- gbm(medv ~ ., data = Boston[train,], distribution = "gaussian",
                    n.trees = 5000, interaction.depth=4)
summary(boost.boston)
```

Yukarıda `summary()` fonksiyonu her bir değişken için göreceli etki istatistiklerini ve grafiğini vermektedir. Buna göre, önceki sonuçlarla uyumlu olarak, `lstat` ve `rm` en önemli değişkenlerdir. 

Buna ek olarak değişkenlerin kısmi etkilerini de görselleştirebiliriz:
```{r}
par(mfrow=c(1,2))
plot(boost.boston,i="rm")
plot(boost.boston,i="lstat")
```

Kısmi bağımlılık grafikleri diğer değişkenlerin etkisi arındırıldıktan sonra bir değişkenin çıktı üzerindeki marjinal etkisini göstermektedir. Buna göre medyan ev fiyatları `rm`'ye göre artarken, `lstat` değişkenine göre azalmaktadır. 

Şimdi takviyeli (boosted) ağaç modeline göre `medv` değişkenini test verisinde tahmin edelim
```{r}
yhat.boost <- predict(boost.boston, newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost-boston.test)^2)
```

Yukarıda default $\lambda = 0.001$ (küçültme, shrinkage) parametresi kullanıldı. $\lambda=0.2$ için yeniden tahmin edelim: 
```{r}
boost.boston <- gbm(medv ~ ., data = Boston[train,],
                    distribution = "gaussian", n.trees=5000,
                    interaction.depth = 4, shrinkage = 0.2, verbose=F)
yhat.boost <- predict(boost.boston, newdata=Boston[-train,], n.trees=5000)
mean((yhat.boost-boston.test)^2)
```

Bu durumda MSE daha yüksek çıkmıştır. 

<br/>

# Örnek: Titanic 

Bu örnekte Titanic kazasında hayatını kaybedenler için bir sınıflandırma ağacı oluşturmaya çalışacağız (detaylar için bkz. [RMS Titanic](https://en.wikipedia.org/wiki/RMS_Titanic) ). Bu trajik kaza çok fazla kişinin ölümü ile sonuçlanmıştır. Böyle kazaların tekrar yaşanmaması için çok sayıda güvenlik tedbirlerinin alınmasında ve yasal düzenleme yapılmasında etkili olmuştur. 

Sorular: acaba bu kadar kişinin ölümünün ardındaki nedenler nelerdir? Hangi değişkenler bireyin hayatta kalma olasılığı üzerinde önemli bir etkiye sahiptir?

Önce verileri `R`'a tanıtalım: 
```{r, warning=FALSE}
library(tidyverse)
library(modelr)
library(broom)
set.seed(1234)

theme_set(theme_minimal())

library(titanic)
# use the training data 
titanic <- titanic_train %>%
  as_tibble()

titanic %>%
  head() %>%
  knitr::kable()
```


Çıktı değişkenini oluştur: 
```{r}
titanic_tree_data <- titanic %>%
  mutate(Survived = if_else(Survived == 1, "Survived", "Died"),
         Survived = as.factor(Survived),
         Sex = as.factor(Sex))
titanic_tree_data
table(titanic_tree_data$Survived)
```

Sadece  `age` ve `sex` özniteliklerini kullanarak küçük bir ağaç tahmin edelim. Bunun için `partykit` paketini kullanacağız: 
```{r, warning=FALSE}
library(partykit)
titanic_tree <- ctree(Survived ~ Age + Sex, data = titanic_tree_data)
titanic_tree
```

Oluşturduğumuz ağaç 3 son düğüme (yaprak) ve 2 iç düğüme sahiptir. Ağacın görselleştirilmesi: 
```{r}
plot(titanic_tree)
```

Sınıflandırma hatası: 
```{r}
# compute predicted values
pred1 <- predict(titanic_tree) 
# true classifications
predsuccess1 <- (pred1 == titanic_tree_data$Survived)
# wrong classifications
prederror1 <- !predsuccess1
# average misclassification rate: 
mean(prederror1, na.rm = TRUE)

# or just in one line: 
# 1 - mean(predict(titanic_tree) == titanic_tree_data$Survived, na.rm = TRUE)
```

Bu basit ağaçla bile hata oranı yaklaşık % 21. 

Veri setinde yer alan diğer öznitelikleri de kullanarak daha karmaşık bir ağaç tahmin edelim. Önce karakter değişkenleri faktör değişkenine dönüştürelim: 
```{r}
titanic_tree_full_data <- titanic %>%
  mutate(Survived = if_else(Survived == 1, "Survived",
                            if_else(Survived == 0, "Died", NA_character_))) %>%
  mutate_if(is.character, as.factor)
```

Ağacı tahmin et:  
```{r}

titanic_tree_full <- ctree(Survived ~ Pclass + Sex + Age + SibSp +
                             Parch + Fare + Embarked,
                           data = titanic_tree_full_data)
titanic_tree_full

```

```{r, echo=TRUE, results="hold"}
plot(titanic_tree_full,  ip_args = list(pval = FALSE, id = FALSE),
     tp_args = list( id = FALSE) )
```

Hata oranı: 
```{r, echo=TRUE, results="hold"}
# error rate
1 - mean(predict(titanic_tree_full) == titanic_tree_data$Survived,
         na.rm = TRUE)
```

Hata oranı daha da düştü, %18.3. 


Şimdi `caret` paketini kullanarak bir rassal orman tahmin edelim. Bunun için  `caret::train()` fonksiyonunu kullanacağız:  
```{r}
# drop the NA obs.
titanic_rf_data <- titanic_tree_full_data %>%
  select(Survived, Pclass, Sex, Age, SibSp, Parch, Fare, Embarked) %>%
  drop_na()
titanic_rf_data
```

```{r, warning=FALSE}
# build a random forest with ntree=200
library(caret)
titanic_rf <- train(Survived ~ ., data = titanic_rf_data,
                    method = "rf",
                    ntree = 200,
                    trControl = trainControl(method = "oob"))
titanic_rf
```



```{r, echo=TRUE, results="hold"}
str(titanic_rf, max.level = 1)
# Info on the final model: 
titanic_rf$finalModel
```

```{r, echo=TRUE, results="hold"}
# confusion table 
knitr::kable(titanic_rf$finalModel$confusion)
```



```{r, echo=TRUE, results="hold"}
# look at an individual tree
randomForest::getTree(titanic_rf$finalModel, labelVar = TRUE)
```



```{r, echo=TRUE, results="hold"}
# var importance plot 
randomForest::varImpPlot(titanic_rf$finalModel)
```



